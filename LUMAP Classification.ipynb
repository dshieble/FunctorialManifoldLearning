{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sklearn.manifold import spectral_embedding, MDS, SpectralEmbedding\n",
    "from scipy.spatial.distance import squareform, pdist, cdist\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "from scipy.spatial.distance import cdist \n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit, jit\n",
    "from numba import types\n",
    "from numba.typed import Dict\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from matplotlib.colors import BASE_COLORS\n",
    "from helpers import (\n",
    "    mds, write_embedding_to_text_file, write_embedding_to_two_text_files, is_numeric, fit_laplacian_eigenmaps\n",
    ")\n",
    "import lumap\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "sns.reset_defaults()\n",
    "sns.set_context(context='talk',font_scale=0.7)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import vstack \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def get_news_dataset():\n",
    "    train_bunch = fetch_20newsgroups(subset=\"train\")\n",
    "    test_bunch = fetch_20newsgroups(subset=\"test\")\n",
    "    raw_Xtrain, raw_ytrain = train_bunch['data'], train_bunch['target']\n",
    "    raw_Xtest, raw_ytest = test_bunch['data'], test_bunch['target']\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('svd', TruncatedSVD(n_components=500)),\n",
    "    ])\n",
    "\n",
    "    raw_Xtrain = raw_Xtrain[::3]\n",
    "    raw_Xtest = raw_Xtest[::3]\n",
    "    raw_ytrain = raw_ytrain[::3]\n",
    "    raw_ytest = raw_ytest[::3]\n",
    "    rawX = raw_Xtrain + raw_Xtest\n",
    "    rawX = pipeline.fit_transform(rawX)\n",
    "    raw_Xtrain, raw_Xtest = rawX[:len(raw_ytrain)], rawX[len(raw_ytrain):]\n",
    "    return rawX, raw_Xtrain, raw_ytrain, raw_Xtest, raw_ytest\n",
    "\n",
    "def get_mnist_dataset():\n",
    "    raw_Xtrain, raw_ytrain = load_mnist('data/fashion', kind='train')\n",
    "    raw_Xtest, raw_ytest = load_mnist('data/fashion', kind='t10k')\n",
    "\n",
    "    raw_Xtrain = raw_Xtrain[::3]\n",
    "    raw_Xtest = raw_Xtest[::3]\n",
    "    raw_ytrain = raw_ytrain[::3]\n",
    "    raw_ytest = raw_ytest[::3]\n",
    "    rawX = np.vstack((raw_Xtrain, raw_Xtest))\n",
    "    return rawX, raw_Xtrain, raw_ytrain, raw_Xtest, raw_ytest\n",
    "\n",
    "\n",
    "dataset_fn_dict = {\n",
    "    \"news\": get_news_dataset,\n",
    "    \"mnist\": get_mnist_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.distances as dist\n",
    "\n",
    "def get_pca_embeddings(rawX, train_count, n_components):\n",
    "    raw_pca_embeddings = PCA(n_components=n_components).fit_transform(rawX)\n",
    "    pca_embeddings = Normalizer().fit_transform(raw_pca_embeddings)\n",
    "    train_pca_embeddings = pca_embeddings[:train_count]\n",
    "    test_pca_embeddings = pca_embeddings[train_count:]  \n",
    "    return train_pca_embeddings, test_pca_embeddings\n",
    "\n",
    "def get_fumap_embeddings(rawX, train_count, n_components, n_neighbors):\n",
    "    embeddings = lumap.fit_umap(\n",
    "        X=rawX,\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        metric=\"euclidean\")\n",
    "    embeddings = Normalizer().fit_transform(embeddings)\n",
    "    train_embeddings = embeddings[:train_count]\n",
    "    test_embeddings = embeddings[train_count:]\n",
    "    return train_embeddings, test_embeddings\n",
    "\n",
    "def get_lumap_embeddings(rawX, train_count, n_components, n_neighbors):\n",
    "    embeddings = lumap.fit_lumap(\n",
    "        X=rawX,\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        metric=\"euclidean\")\n",
    "    embeddings = Normalizer().fit_transform(embeddings)\n",
    "    train_embeddings = embeddings[:train_count]\n",
    "    test_embeddings = embeddings[train_count:]\n",
    "    return train_embeddings, test_embeddings\n",
    "\n",
    "\n",
    "def train_model(Xtrain, ytrain, Xtest, ytest):\n",
    "    results = {}\n",
    "    for name, model in [(\"KNeighborsClassifier\", KNeighborsClassifier(n_neighbors=5))]:\n",
    "        model.fit(Xtrain, ytrain)\n",
    "        results[name] = {}\n",
    "\n",
    "        encoder = OneHotEncoder()\n",
    "        encoder.fit(ytrain[:, None])\n",
    "        results[name][\"train_score\"] = roc_auc_score(\n",
    "            encoder.transform(ytrain[:, None]).todense(),\n",
    "            encoder.transform(model.predict(Xtrain)[:, None]).todense())\n",
    "\n",
    "        results[name][\"test_score\"] = roc_auc_score(\n",
    "            encoder.transform(ytest[:, None]).todense(),\n",
    "            encoder.transform(model.predict(Xtest)[:, None]).todense())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "news\n",
      "==========================\n",
      "==========================\n",
      "building lumap embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/numba/np/ufunc/parallel.py:355: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 11000. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training lumap model\n",
      "building umap embeddings\n",
      "training fumap model\n",
      "n_components: 25\n",
      "lumap_results {'KNeighborsClassifier': {'train_score': 0.8234079267459984, 'test_score': 0.716167050306576}}\n",
      "fumap_results {'KNeighborsClassifier': {'train_score': 0.8477579266462195, 'test_score': 0.7275056802105252}}\n",
      "iteration time: 48.8411021232605\n",
      "==========================\n",
      "building lumap embeddings\n",
      "training lumap model\n",
      "building umap embeddings\n",
      "training fumap model\n",
      "n_components: 50\n",
      "lumap_results {'KNeighborsClassifier': {'train_score': 0.8367311899050754, 'test_score': 0.7112014269133803}}\n",
      "fumap_results {'KNeighborsClassifier': {'train_score': 0.8438868389106476, 'test_score': 0.7258545711160274}}\n",
      "iteration time: 50.50781989097595\n",
      "==========================\n",
      "mnist\n",
      "==========================\n",
      "==========================\n",
      "building lumap embeddings\n",
      "training lumap model\n",
      "building umap embeddings\n",
      "training fumap model\n",
      "n_components: 25\n",
      "lumap_results {'KNeighborsClassifier': {'train_score': 0.9130783962079565, 'test_score': 0.8819915359884325}}\n",
      "fumap_results {'KNeighborsClassifier': {'train_score': 0.9184987588409627, 'test_score': 0.8863215716761088}}\n",
      "iteration time: 93.64368104934692\n",
      "==========================\n",
      "building lumap embeddings\n",
      "training lumap model\n",
      "building umap embeddings\n",
      "training fumap model\n",
      "n_components: 50\n",
      "lumap_results {'KNeighborsClassifier': {'train_score': 0.92059242500614, 'test_score': 0.8887298259012185}}\n",
      "fumap_results {'KNeighborsClassifier': {'train_score': 0.918977884772706, 'test_score': 0.8872191416335822}}\n",
      "iteration time: 104.62972784042358\n"
     ]
    }
   ],
   "source": [
    "all_n_components = [25, 50]\n",
    "all_results = {}\n",
    "\n",
    "for dataset_name, dataset_fn in dataset_fn_dict.items():\n",
    "    print(\"==========================\")\n",
    "    print(dataset_name)\n",
    "    print(\"==========================\")\n",
    "    rawX, raw_Xtrain, raw_ytrain, raw_Xtest, raw_ytest = dataset_fn()\n",
    "    all_results[dataset_name] = {}\n",
    "    for n_components in all_n_components:\n",
    "        start = time.time()\n",
    "        print(\"==========================\")\n",
    "        result = {}\n",
    "\n",
    "        print(\"building lumap embeddings\")\n",
    "        train_lumap_embeddings, test_lumap_embeddings = get_lumap_embeddings(\n",
    "            rawX=rawX,\n",
    "            train_count=raw_Xtrain.shape[0],\n",
    "            n_components=n_components,\n",
    "            n_neighbors=15)\n",
    "        print(\"training lumap model\")\n",
    "        result[\"lumap_results\"] = train_model(\n",
    "            Xtrain=train_lumap_embeddings,\n",
    "            ytrain=raw_ytrain,\n",
    "            Xtest=test_lumap_embeddings,\n",
    "            ytest=raw_ytest)\n",
    "\n",
    "        print(\"building umap embeddings\")\n",
    "        train_fumap_embeddings, test_fumap_embeddings = get_fumap_embeddings(\n",
    "            rawX=rawX,\n",
    "            train_count=raw_Xtrain.shape[0],\n",
    "            n_components=n_components,\n",
    "            n_neighbors=15)\n",
    "        print(\"training fumap model\")\n",
    "        result[\"fumap_results\"] = train_model(\n",
    "            Xtrain=train_fumap_embeddings,\n",
    "            ytrain=raw_ytrain,\n",
    "            Xtest=test_fumap_embeddings,\n",
    "            ytest=raw_ytest)\n",
    "\n",
    "        print(\"n_components: {}\".format(n_components))\n",
    "        for k, v in result.items():\n",
    "            print(k, v)\n",
    "\n",
    "        all_results[dataset_name][n_components] = result\n",
    "        print(\"iteration time: {}\".format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
